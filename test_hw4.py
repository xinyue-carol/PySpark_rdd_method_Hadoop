# -*- coding: utf-8 -*-
"""test_hw4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jyoQEKe49YmqhklTOrE6mMON-We_FO6d
"""


from pyspark import SparkContext


from numpy import std
from numpy import median
import datetime
import json
import csv
if __name__=='__main__':
    sc = SparkContext()

    core='hdfs:///data/share/bdm/core-places-nyc.csv'
    nyc_rest='hdfs:///data/share/bdm/weekly-patterns-nyc-2019-2020/*'

    #def funtions:
    def geteveryday(x):
        date_list = []
        begin_date = datetime.datetime.strptime(x[1][:10], "%Y-%m-%d")
        end_date = datetime.datetime.strptime(x[2][:10], "%Y-%m-%d")
        while begin_date + datetime.timedelta(days=1) <= end_date:
            date_str = str(begin_date.year) + '-' + str(begin_date.month) + '-' + str(begin_date.day)
            date_list.append(date_str)
            begin_date += datetime.timedelta(days=1)
        return (x[0],x[3],date_list)

    def combine(x):
      combinelist=zip(x[2],json.loads(x[1]))
      return tuple(combinelist)

    #select data needed in core csv
    coreid=sc.textFile(core)\
             .map(lambda x: next(csv.reader([x])))\
             .map(lambda x: (x[1], x[9]))\
             .cache()
    rest_list=set(coreid.filter(lambda x: x[1] in ['722511'])\
               .map(lambda x:x[0])\
               .collect())
    #select needed columns in weeklu patterns
    data0=sc.textFile(nyc_rest)\
      .map(lambda x: next(csv.reader([x])))\
      .map(lambda x: (x[1],x[12],x[13], x[16]))\
      .cache()


    #process data for 9 sections
    rest_data=data0.filter(lambda x: x[0] in rest_list)\
                    .map(geteveryday)\
                    .flatMap(combine)\
                    .groupByKey()\
                    .mapValues(list)\
                    .map(lambda x: (x[0][:4],datetime.datetime.strptime(x[0],"%Y-%m-%d") ,int((median(x[1])+std(x[1]))), int((median(x[1])-std(x[1]))), int(median(x[1]))))\
                    .map(lambda x: x if x[3]>=0 else (x[0],x[1], x[2], 0, x[4]))\
                    .map(lambda x:  x if x[0]=='2020' else (x[0], x[1].replace(year='2020'), x[2],x[3],x[4]))\
                   .cache()

    #sort and covert rdd to df with right column names
    rest_data_sort=rest_data.sortBy(lambda x:x[1]).sortBy(lambda x:x[0])
    df_rest = spark.createDataFrame(rest_data_sort, ['year', 'date','high','low','median'])

    df_rest.write.option("header", "true").csv('full_service_restaurants.csv')
